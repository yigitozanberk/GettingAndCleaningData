---
title: "RP4_W_1"
author: "yob"
date: "8 Apr 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.



## Getting and Cleaning Data 
## Week 1

#Content

Data collection

Raw files (.csv,.xlsx)
Databases (mySQL)
APIs

Data formats

Flat files (.csv,.txt)
XML
JSON

Making data tidy

Distributing data

Scripting for data cleaning


#About the course

- finding and extracting raw data
-tidy data principles and how to make data tiny
-practical implementation through a range of R packages

data.baltimorecity.gov

all free information about baltimore city

raw data -> processing script -> tidy data -> data analysis -> data communication


you need to organize the data into a tidy format before analyzing it.

#RAW and PROCESSED DATA

Data are values of qualitative or quantitative variables, belonging to a set of items.

qualitative : country of origin, sex, treatment

quantitative : height, weight,  blood pressure

blood measure is measured by calculating the pressure measurement.

* raw data may only need to be processed once

*processing can include merging, subsetting, transforming, etc.

*there may be standards for processing

*all steps should be recorded


## Components of Tidy Data

the raw data

a tidy data set

a code book describing each variable and its values in the tidy data set

an explicit and exact recipe you used to go from 1 -> 2 -> 3

#Tidy data

each variable you measure should be in one column

each different observation of that variable should be in a different row

there should be one table for each "kind" of variable

if you have multiple tables, they should include a column in the table that allows them to be linked

-- 

include a row at the top of each file with variable names

make variable names human readable AgeAtDiagnosis instead of AgeDx

in general data should be saved in one file per table



# The code book

information about the variables including units, in the data set not contained in the tidy data

information abotu the summary choices you made

information about the experimental study design you used

---

a common format for this document is a Word/Text file

there should be a section called "Study design" that has a thorough description of how you collected the data

there must be a section called "Code Book" that describes each variable and its units


# The instruction list

ideally a computer script in R, but Python is ok too

the input for the script is the raw data

the output is the processed, tidy data

there are no parameters to the script

---

in some cases it will not be possible to script every step, in that case you should provide instructions like:

step1 : take the raw file, run version 3.1.2 of summarize software with parameters a = 1, b = 2, c = 3

step2 : run the software separately for each sample

step3 : take colum three of outputfile out for each sample and that is the corresponding row in the output data set

## DOWNLOADING DATA

getwd()

setwd()
relative : setwd("./data") for going one level deeper
setwd("../") for going one level upwards

absolute : setwd("/Users/username/data/")

file.exists("directoryName") will check to see if the directory exists

dir.create("directoryName") will create a directory if it doesn't exist

if(!file.exists("data")){
        dir.create("data")
}

#download.file()

param:
url
destfile
method

useful for downloading tab-delimited, csv, and other files.





```{r}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"

download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl") #method = curl is necessary for macs
list.files("./data")

dateDownloaded <- date() #be sure to record when you downloaded

```

if the url starts with http you can use download.file()

if the url starts with https on Windows you may be ok

if the url starts with https on Mac you may need to set method = "curl"

if the file is big, this might take a while

## READING LOCAL FLAT FILES

done in R programming class

read.table()

and read.csv()

and read.csv2()

important params:
file, header, sep, row.names, nrows

```{r}
cameraData <- read.csv("./data/cameras.csv")
```

quote : you can tell R whether there are any quoted values quote="" means no quotes

na.strings - set the character that represents a missing value

nrows - how many rows to read
skip : number of lines to skip before starting to read

biggest trouble with reading flat files are quotation marks ' or " placed in data values, setting quote = "" often resolves these.

# READING EXCEL FILES

```{r}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"

download.file(fileUrl, destfile = "./data/cameras.xlsx", method = "curl")

dateDownloaded <- date()



```


you can read specific rows and specific columns
with
colIndex <- 2:3

rowIndex <- 1:2

##Swirl Courses
Intermediate

Regression Models: The basics of regression modeling in R
Getting and Cleaning Data: dplyr, tidyr, lubridate, oh my!

Advanced

Statistical Inference: This intermediate to advanced level course closely follows the Statistical Inference course of the Johns Hopkins Data Science Specialization on Coursera. It introduces the student to basic concepts of statistical inference including probability, hypothesis testing, confidence intervals and p-values. It concludes with an initiation to topics of particular relevance to big data, issues of multiple testing and resampling.

##Reading XML

extensible markup language

frequently used to store structured data

components :
-markup - labels that give the text structure
-content - the actual text of the document

start tags <section>
end tags </section>
empty tags <line-break />

<greeting> Hello, wrold </greeting>

<img src="jeff.jgp" alt="instructor"/>
<step number="3"> Connect A to B. </step>

en.wikipedia.org/wiki/XML

```{r}
library(XML)
library(RCurl)

fileUrl <- getURL("https://www.w3schools.com/xml/simple.xml")
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
rootNode[[1]] #same way you access a list in R
rootNode[[1]][[1]] #subsetting to go one level down

xmlSApply(rootNode, xmlValue) #parse an xml object, and give a function to execute
#rootNode contains the entire document.



```
for XPath language 

go to 
http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf


```{r}
xpathSApply(rootNode, "//name", xmlValue)
#gets all of the nodes that correspond to an element titled "name"

xpathSApply(rootNode, "//price", xmlValue)


```
http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens


```{r}
fileUrl <- getURL("http://www.espn.com/nfl/team/_/name/bal/baltimore-ravens")

doc <- htmlTreeParse(fileUrl, useInternal=TRUE)#useInternal to get all internal files
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class='team-name']", xmlValue)
scores

```

there are XML tutorials on the XML package

##READING JSON

javascript object notation

lightweight data storage

common format for data from application programming interfaces

similar structure to XML but different syntax

data are stored as
 numbers
 strings
 boolean
 array
 object
 
```{r}
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)

names(jsonData$owner)

jsonData$owner$login

myjson <- toJSON(iris, pretty = TRUE) #turning into json
cat(myjson)#to print out

iris2 <- fromJSON(myjson)
head(iris2)


```
 
www.json.org

jsonlite tutorial

jsonlite vignette



## data.table package
**another syntax, faster use**

inherets from data.frame
-all functions that accept data.frame work on data.table

written in C so it's much faster

much faster at subsetting, group, and updating

```{r}
library(data.table)
DF = data.frame(x = rnorm(9), y = rep(c("a", "b", "c"), each = 3), z = rnorm(9))

head(DF, 3)

DT = data.table(x = rnorm(9), y = rep(c("a", "b", "c"), each = 3), z = rnorm(9))
head(DT,3)

tables() #see all data frames in memory

DT[2,]

DT[DT$y=="a", ]

DT[c(2,3)] #takes 2nd and 3rd rows

DT[, c(2,3)]





```


subsetting function is modified for data.table

argument you pass after the comma is called an expression

in R an expression is a collection of statements enclosed in curley brackets

```{r}
DT[, list(mean(x), sum(z))]
DT[, table(y)] #you can perform any function after the comma

DT[, w:=z^2]

DT2 <- DT #still dependent. if you want independent copy you 
# need the copy function

DT[, m:= {tmp <- (x + z); log2(tmp+5)}]
DT[, a:=x>0]
DT[, b:= mean(x+w), by=a]



```
.N is an integer length 1, containing the number

```{r}
set.seed(123)
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT[,.N, by=x] #if you want to count the times each of those letters appear


```

KEYS

```{r}
DT <- data.table(x=rep(c("a", "b", "c"), each=100), y = rnorm(300))
setkey(DT, x)
DT['a']#it knows to go and look in the key

#easy merging

DT1 <- data.table(x=c('a', 'a', 'b', 'dt1'), y=1:4)
DT2 <- data.table(x=c('a', 'b', 'dt2'), z=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)
#vuala!


```
you can use keys to facilitate joins


very fast reading.

much faster to read files with read.table


check
raphael gottardo's github account